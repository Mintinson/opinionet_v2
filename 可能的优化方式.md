### 新增配置开关

| 参数                       | 默认值   | 说明                                   |
| ------------------------ | ----- | ------------------------------------ |
| `--use_biaffine`         | False | 启用Biaffine Pointer Network           |
| `--biaffine_hidden_size` | 150   | Biaffine隐藏层大小                        |
| `--use_rdrop`            | False | 启用R-Drop正则化                          |
| `--rdrop_alpha`          | 0.3   | R-Drop KL散度权重                        |
| `--label_smoothing`      | 0.0   | Label Smoothing系数 (0.1推荐)            |
| `--focal`                | False | 启用 focal loss （与 Label Smoothing 互斥） |
| `--use_fgm`              | False | 启用FGM对抗训练                            |
| `--fgm_epsilon`          | 1.0   | FGM扰动大小                              |
| `--nms_threshold`        | 0.1   | NMS过滤阈值                              |

### 新增Large模型

|模型名|HuggingFace路径|学习率|
|---|---|---|
|`roberta_large`|`hfl/chinese-roberta-wwm-ext-large`|3e-6|
|`macbert_large`|`hfl/chinese-macbert-large`|3e-6|
|`ernie_large`|`nghuyong/ernie-3.0-xbase-zh`|4e-6|

```bash
# 使用Large模型 + 全部优化
train_model --base_model roberta_large --batch_size 4 \
  --use_biaffine true --use_rdrop true --label_smoothing 0.1 --use_fgm true

```

---

上述方法可以组合尝试排列组合尝试以下，也可以一次只增加一个配置，看看有没有效果。

---

另一种数据增广的方法：back-translation —— 将评价翻译成英文再翻译回中文。代码在 [back_translate.py](opinionet/data/back_translate.py), 不知道会不会提升模型的能力，还没尝试过。也可以将其与原有的增广方法组合在一起（可能会很花时间）
